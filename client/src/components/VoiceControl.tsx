import {
  convertBlobToBase64,
  ensureSingleValidAudioTrack,
  EVIWebAudioPlayer,
  getAudioStream,
  getBrowserSupportedMimeType,
  Hume,
  HumeClient,
  MimeType,
} from 'hume';
import { useCallback, useEffect, useRef, useState } from 'react';
import './VoiceControl.css';

// Type aliases for Hume SDK types
type SubscribeEvent = Hume.empathicVoice.SubscribeEvent;
type ChatSocket = Awaited<ReturnType<HumeClient['empathicVoice']['chat']['connect']>>;

interface VoiceControlProps {
  userId: string;
  onStateChange?: (state: {
    isConnected: boolean;
    isListening: boolean;
    isSpeaking: boolean;
  }) => void;
  onEmotionsDetected?: (emotions: Array<{ name: string; score: number }>) => void;
  onConnectRef?: React.MutableRefObject<(() => void) | null>;
  onDisconnectRef?: React.MutableRefObject<(() => void) | null>;
}

const FRAME_INTERVAL_MS = 1000; // Send a frame every 1 second

// Get API base URL for server calls (vision, etc.)
const getApiBaseUrl = () => {
  if (import.meta.env?.DEV) {
    return 'http://localhost:3000';
  }
  return '';
};

const VoiceControl = ({
  userId: _userId,
  onStateChange,
  onEmotionsDetected,
  onConnectRef,
  onDisconnectRef,
}: VoiceControlProps) => {
  const [isConnected, setIsConnected] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [isCameraOn, setIsCameraOn] = useState(false);
  const [showPictureModal, setShowPictureModal] = useState(false);
  const [showFlash, setShowFlash] = useState(false);
  const [transcript, setTranscript] = useState<string[]>([]);
  const [_status, setStatus] = useState('Tap avatar to connect');
  const [topEmotions, setTopEmotions] = useState<Array<{ name: string; score: number }>>([]);

  // Trigger camera flash effect
  const triggerFlash = useCallback(() => {
    setShowFlash(true);
    setTimeout(() => setShowFlash(false), 300);
  }, []);

  // Camera refs
  const videoRef = useRef<HTMLVideoElement | null>(null);
  const pictureVideoRef = useRef<HTMLVideoElement | null>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null);
  const cameraStreamRef = useRef<MediaStream | null>(null);
  const pictureStreamRef = useRef<MediaStream | null>(null);
  const frameIntervalRef = useRef<number | null>(null);

  // Hume SDK refs
  const socketRef = useRef<ChatSocket | null>(null);
  const recorderRef = useRef<MediaRecorder | null>(null);
  const playerRef = useRef<EVIWebAudioPlayer | null>(null);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      disconnect();
    };
  }, []);

  // Notify parent component of state changes
  useEffect(() => {
    if (onStateChange) {
      onStateChange({ isConnected, isListening, isSpeaking });
    }
  }, [isConnected, isListening, isSpeaking, onStateChange]);

  // Expose connect/disconnect functions to parent
  useEffect(() => {
    if (onConnectRef) {
      onConnectRef.current = connect;
    }
    if (onDisconnectRef) {
      onDisconnectRef.current = disconnect;
    }
  });

  // Start audio capture using Hume SDK utilities
  const startAudioCapture = useCallback(async (socket: ChatSocket) => {
    const mimeTypeResult = getBrowserSupportedMimeType();
    const mimeType = mimeTypeResult.success ? mimeTypeResult.mimeType : MimeType.WEBM;

    const micAudioStream = await getAudioStream();
    ensureSingleValidAudioTrack(micAudioStream);

    const recorder = new MediaRecorder(micAudioStream, { mimeType });
    recorder.ondataavailable = async (e: BlobEvent) => {
      if (e.data.size > 0 && socket.readyState === WebSocket.OPEN) {
        const data = await convertBlobToBase64(e.data);
        socket.sendAudioInput({ data });
      }
    };
    recorder.onerror = (e) => console.error('MediaRecorder error:', e);
    recorder.start(100); // Send audio every 100ms

    recorderRef.current = recorder;
    setIsListening(true);
    console.log('üé§ Audio capture started (Hume SDK)');

    return recorder;
  }, []);

  // Stop audio capture
  const stopAudioCapture = useCallback(() => {
    if (recorderRef.current) {
      recorderRef.current.stream.getTracks().forEach((t) => t.stop());
      recorderRef.current.stop();
      recorderRef.current = null;
    }
    setIsListening(false);
  }, []);

  // Handle messages from Hume EVI
  const handleMessage = useCallback(
    async (message: SubscribeEvent) => {
      // Debug: log ALL messages
      console.log('üì® RAW MESSAGE:', message.type, JSON.stringify(message).slice(0, 500));

      switch (message.type) {
        case 'chat_metadata':
          console.log('üìã Chat ID:', message.chatId);
          break;

        case 'user_message': {
          let content = message.message?.content || '';
          // Strip [SYSTEM CONTEXT: ...] prefix if present
          const contextMatch = content.match(/^\[SYSTEM CONTEXT:.*?\]\s*/);
          if (contextMatch) {
            content = content.substring(contextMatch[0].length);
          }
          if (content) {
            setTranscript((prev) => [...prev, `You: ${content}`]);

            // Detect "take a picture" voice command and capture from expression camera
            const lowerContent = content.toLowerCase();
            if (
              lowerContent.includes('take a picture') ||
              lowerContent.includes('take picture') ||
              lowerContent.includes('show you') ||
              lowerContent.includes('what do you see') ||
              lowerContent.includes('look at this')
            ) {
              // Capture from the expression camera if it's running
              if (videoRef.current && canvasRef.current && isCameraOn) {
                const video = videoRef.current;
                const canvas = canvasRef.current;
                const ctx = canvas.getContext('2d');
                if (ctx) {
                  canvas.width = video.videoWidth || 640;
                  canvas.height = video.videoHeight || 480;
                  ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                  const dataUrl = canvas.toDataURL('image/jpeg', 0.9);
                  const base64Data = dataUrl.split(',')[1];
                  sendImageToServer(base64Data, 'picture');
                  console.log('üì∏ Voice-triggered picture captured and sent');
                }
              }
            }
          }
          // Extract emotions from prosody if available
          if (message.models?.prosody?.scores) {
            const scores = message.models.prosody.scores as unknown as Record<string, number>;
            const sorted = Object.entries(scores)
              .sort(([, a], [, b]) => b - a)
              .slice(0, 3)
              .map(([name, score]) => ({ name, score }));
            setTopEmotions(sorted);
            if (onEmotionsDetected) {
              onEmotionsDetected(sorted);
            }
          }
          break;
        }

        case 'assistant_message': {
          const content = message.message?.content || '';
          if (content) {
            setTranscript((prev) => [...prev, `NoVo: ${content}`]);
          }
          setIsSpeaking(true);
          setIsListening(false);
          break;
        }

        case 'audio_output':
          // Use Hume's EVIWebAudioPlayer for smooth playback
          if (playerRef.current) {
            await playerRef.current.enqueue(message);
          }
          break;

        case 'user_interruption':
          // User interrupted - stop playback
          if (playerRef.current) {
            playerRef.current.stop();
          }
          setIsSpeaking(false);
          setIsListening(true);
          break;

        case 'assistant_end':
          setIsSpeaking(false);
          setIsListening(true);
          break;

        case 'error':
          console.error('Hume error:', message);
          break;

        case 'tool_call': {
          const toolName = (message as { name?: string }).name;
          const toolCallId = (message as { toolCallId?: string }).toolCallId;
          console.log('üîß Tool call:', toolName, toolCallId);

          if (toolName === 'web_search') {
            setTranscript((prev) => [...prev, 'üîç Searching the web...']);
          }

          if (toolName === 'take_picture' && toolCallId) {
            setTranscript((prev) => [...prev, 'üì∏ Taking a picture...']);
            
            if (videoRef.current && canvasRef.current && isCameraOn) {
              const video = videoRef.current;
              const canvas = canvasRef.current;
              const ctx = canvas.getContext('2d');
              if (ctx) {
                canvas.width = video.videoWidth || 640;
                canvas.height = video.videoHeight || 480;
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                const dataUrl = canvas.toDataURL('image/jpeg', 0.9);
                const base64Data = dataUrl.split(',')[1];

                // Analyze image on server
                fetch(`${getApiBaseUrl()}/api/vision/analyze`, {
                  method: 'POST',
                  headers: { 'Content-Type': 'application/json' },
                  body: JSON.stringify({ image: base64Data, type: 'picture' }),
                })
                  .then((res) => res.json())
                  .then((result) => {
                    if (result.success && result.data?.context && socketRef.current) {
                      console.log('‚úÖ Vision analysis complete:', result.data.context.substring(0, 100));
                      socketRef.current.sendToolResponseMessage({
                        toolCallId,
                        content: result.data.context,
                      });
                    } else {
                      console.error('‚ùå Vision analysis failed:', result);
                      if (socketRef.current) {
                        socketRef.current.sendToolErrorMessage({
                          toolCallId,
                          error: 'Failed to analyze the image. Please try again.',
                          content: '',
                        });
                      }
                    }
                  })
                  .catch((error) => {
                    console.error('‚ùå Error analyzing image:', error);
                    if (socketRef.current) {
                      socketRef.current.sendToolErrorMessage({
                        toolCallId,
                        error: 'Network error while analyzing the image.',
                        content: '',
                      });
                    }
                  });
                triggerFlash();
                console.log('üì∏ Picture captured for tool call');
              }
            } else {
              console.warn('‚ö†Ô∏è Camera not enabled for take_picture tool');
              if (socketRef.current && toolCallId) {
                socketRef.current.sendToolErrorMessage({
                  toolCallId,
                  error: 'Camera is not enabled. Please turn on "Let NoVo See Me" first.',
                  content: '',
                });
              }
            }
          }
          break;
        }

        default:
          console.log('üì• Hume:', message.type, message);
      }
    },
    [onEmotionsDetected, isCameraOn, triggerFlash]
  );

  // Send image to server for vision analysis (camera or picture)
  const sendImageToServer = useCallback(
    async (imageData: string, type: 'camera_frame' | 'picture') => {
      try {
        const response = await fetch(`${getApiBaseUrl()}/api/vision/analyze`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ image: imageData, type }),
        });
        const result = await response.json();
        if (result.success && result.data?.context && socketRef.current) {
          // Inject vision context into EVI conversation
          socketRef.current.sendUserInput(result.data.context);
        }
      } catch (error) {
        console.error('Error sending image to server:', error);
      }
    },
    []
  );

  // Capture and send webcam frame for expression analysis
  const captureAndSendFrame = useCallback(() => {
    if (!videoRef.current || !canvasRef.current || !isConnected) return;

    const video = videoRef.current;
    const canvas = canvasRef.current;
    const ctx = canvas.getContext('2d');
    if (!ctx) return;

    canvas.width = video.videoWidth || 320;
    canvas.height = video.videoHeight || 240;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    const dataUrl = canvas.toDataURL('image/jpeg', 0.7);
    const base64Data = dataUrl.split(',')[1];

    sendImageToServer(base64Data, 'camera_frame');
  }, [isConnected, sendImageToServer]);

  // Start webcam for facial expression detection (front camera)
  const startExpressionCamera = useCallback(async () => {
    try {
      if (frameIntervalRef.current) {
        clearInterval(frameIntervalRef.current);
        frameIntervalRef.current = null;
      }
      if (cameraStreamRef.current) {
        cameraStreamRef.current.getTracks().forEach((track) => track.stop());
      }

      const stream = await navigator.mediaDevices.getUserMedia({
        video: { width: 320, height: 240, facingMode: 'user' },
      });
      cameraStreamRef.current = stream;

      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        await videoRef.current.play();
      }

      setIsCameraOn(true);

      // Wait for video to be ready, then capture first frame
      await new Promise((resolve) => setTimeout(resolve, 300));
      captureAndSendFrame();

      // Start sending frames periodically
      frameIntervalRef.current = window.setInterval(captureAndSendFrame, FRAME_INTERVAL_MS);
      console.log('üì∑ Expression camera started');

      // Notify NoVo that she can now see the user
      if (socketRef.current) {
        socketRef.current.sendUserInput(
          '[The user just turned on their camera. You can now see them. Briefly acknowledge that you can see them now and make a friendly observation about them.]'
        );
      }
    } catch (error) {
      console.error('Error starting expression camera:', error);
    }
  }, [captureAndSendFrame]);

  // Open picture modal with rear camera viewfinder
  const openPictureModal = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { width: 640, height: 480, facingMode: 'environment' },
      });
      pictureStreamRef.current = stream;
      setShowPictureModal(true);

      setTimeout(() => {
        if (pictureVideoRef.current) {
          pictureVideoRef.current.srcObject = stream;
          pictureVideoRef.current.play();
        }
      }, 100);
    } catch (error) {
      console.error('Error opening picture camera:', error);
    }
  };

  // Close picture modal
  const closePictureModal = () => {
    if (pictureStreamRef.current) {
      pictureStreamRef.current.getTracks().forEach((track) => track.stop());
      pictureStreamRef.current = null;
    }
    setShowPictureModal(false);
  };

  // Capture picture and send to server for analysis
  const capturePicture = useCallback(() => {
    if (pictureVideoRef.current && canvasRef.current) {
      const video = pictureVideoRef.current;
      const canvas = canvasRef.current;
      const ctx = canvas.getContext('2d');
      if (ctx) {
        canvas.width = video.videoWidth || 640;
        canvas.height = video.videoHeight || 480;
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        const dataUrl = canvas.toDataURL('image/jpeg', 0.9);
        const base64Data = dataUrl.split(',')[1];

        sendImageToServer(base64Data, 'picture');
        console.log('üì∏ Picture captured and sent');
      }
    }
    closePictureModal();
  }, [sendImageToServer]);

  // Stop webcam capture
  const stopWebcam = useCallback(() => {
    if (frameIntervalRef.current) {
      clearInterval(frameIntervalRef.current);
      frameIntervalRef.current = null;
    }
    if (cameraStreamRef.current) {
      cameraStreamRef.current.getTracks().forEach((track) => track.stop());
      cameraStreamRef.current = null;
    }
    if (videoRef.current) {
      videoRef.current.srcObject = null;
    }
    setIsCameraOn(false);
  }, []);

  // Connect to Hume EVI using TypeScript SDK
  const connect = useCallback(async () => {
    try {
      console.log('üîÑ Connect function called...');
      setStatus('Getting access token...');

      // Fetch access token from our server
      console.log('üîë Fetching access token from:', `${getApiBaseUrl()}/api/hume/access-token`);
      const tokenResponse = await fetch(`${getApiBaseUrl()}/api/hume/access-token`);
      const tokenData = await tokenResponse.json();
      console.log('üîë Token response:', tokenData);

      if (!tokenData.success || !tokenData.data?.accessToken) {
        throw new Error('Failed to get access token');
      }

      const { accessToken, configId } = tokenData.data;
      setStatus('Connecting to Hume...');

      // Create Hume client with access token
      const client = new HumeClient({ accessToken });

      // Initialize audio player
      const player = new EVIWebAudioPlayer();
      await player.init();
      playerRef.current = player;

      // Connect to EVI
      console.log('üîå Connecting to EVI with configId:', configId);
      const socket = await client.empathicVoice.chat.connect({
        configId,
      });
      console.log('üîå Socket created:', socket);
      socketRef.current = socket;

      // Set up event handlers
      socket.on('open', async () => {
        console.log('‚úÖ Connected to Hume EVI - WebSocket open');
        setIsConnected(true);
        setStatus('Connected - Speak now!');

        // Send session settings with tool instructions
        socket.sendSessionSettings({
          systemPrompt:
            'You are NoVo, a warm and caring AI companion. IMPORTANT: You must ONLY speak and respond in English. Never use any other language under any circumstances. If you hear something that sounds like another language, assume it is English and respond in English only. Always be warm, friendly, and supportive. You have access to a take_picture tool and web_search tool. When the user asks you to take a picture, look at something, says "what do you see", "look at this", or wants to show you something, you MUST call the take_picture tool. When the user asks for current information, facts, news, or anything requiring real-time data, use the web_search tool to find accurate information. Never say you cannot take pictures or search the web.',
          tools: [
            {
              type: 'function',
              name: 'take_picture',
              description:
                'Captures a photo using the user\'s camera to see what the user is showing. Use this when the user asks you to look at something, take a picture, or says "what do you see", "look at this", "show you something", etc. The camera must be enabled for this to work.',
              parameters: {
                type: 'object',
                properties: {},
                required: [],
              },
            },
          ],
          builtinTools: [{ name: 'web_search' }],
        });

        // Start audio capture
        await startAudioCapture(socket);
      });

      socket.on('message', handleMessage);

      socket.on('error', (err: Error) => {
        console.error('Hume error:', err);
        setStatus('Connection error');
      });

      socket.on('close', () => {
        console.log('üîå Disconnected from Hume');
        setIsConnected(false);
        setIsListening(false);
        setIsSpeaking(false);
        setStatus('Disconnected');
        stopAudioCapture();
        stopWebcam();
      });
    } catch (error) {
      console.error('Error connecting:', error);
      setStatus('Failed to connect');
    }
  }, [handleMessage, startAudioCapture, stopAudioCapture, stopWebcam]);

  // Disconnect from Hume EVI
  const disconnect = useCallback(() => {
    stopWebcam();
    stopAudioCapture();

    if (playerRef.current) {
      playerRef.current.dispose();
      playerRef.current = null;
    }

    if (socketRef.current) {
      socketRef.current.close();
      socketRef.current = null;
    }

    setIsConnected(false);
    setIsListening(false);
    setIsSpeaking(false);
    setTopEmotions([]);
    setStatus('Disconnected');
  }, [stopAudioCapture, stopWebcam]);

  return (
    <div className="voice-control">
      {/* Camera flash overlay */}
      {showFlash && <div className="camera-flash" />}

      {/* Hidden video and canvas for webcam capture */}
      <video ref={videoRef} className="hidden-video" playsInline muted />
      <canvas ref={canvasRef} className="hidden-canvas" />

      {/* Camera buttons */}
      <div className="camera-buttons">
        <button
          type="button"
          className={`camera-btn ${isCameraOn ? 'active' : ''}`}
          onClick={isCameraOn ? stopWebcam : startExpressionCamera}
          disabled={!isConnected}
        >
          {isCameraOn ? 'Stop Seeing Me' : 'Let NoVo See Me'}
          <span className="btn-icon">üëÅÔ∏è</span>
        </button>

        <button
          type="button"
          className="picture-btn"
          onClick={openPictureModal}
          disabled={!isConnected}
        >
          Take a Picture for Me
          <span className="btn-icon">üì∑</span>
        </button>
      </div>

      {/* Picture Modal with viewfinder */}
      {showPictureModal && (
        <div className="picture-modal-overlay">
          <div className="picture-modal">
            <div className="picture-modal-header">
              <h3>üì∑ Frame Your Shot</h3>
              <button type="button" className="close-modal-btn" onClick={closePictureModal}>
                ‚úï
              </button>
            </div>
            <div className="picture-viewfinder">
              <video ref={pictureVideoRef} autoPlay playsInline muted />
            </div>
            <div className="picture-modal-controls">
              <button type="button" className="capture-btn" onClick={capturePicture}>
                üì∏ Capture
              </button>
            </div>
          </div>
        </div>
      )}

      {/* Show detected emotions */}
      {isConnected && isCameraOn && topEmotions.length > 0 && (
        <div className="emotions-display">
          <p className="emotions-label">Your emotions:</p>
          <div className="emotions-list">
            {topEmotions.map((emotion, idx) => (
              <span key={idx} className="emotion-tag">
                {emotion.name}: {(emotion.score * 100).toFixed(0)}%
              </span>
            ))}
          </div>
        </div>
      )}

      {/* Conversation transcript */}
      {transcript.length > 0 && (
        <div className="transcript">
          <h3>Conversation:</h3>
          <div className="transcript-messages">
            {[...transcript].reverse().map((msg, idx) => (
              <div key={idx} className={`message ${msg.startsWith('You:') ? 'user' : 'assistant'}`}>
                {msg}
              </div>
            ))}
          </div>
        </div>
      )}
    </div>
  );
};

export default VoiceControl;
